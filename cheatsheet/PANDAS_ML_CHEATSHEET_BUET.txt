
================================================================================
                       PANDAS CHEATSHEET FOR ML (BUET EXAM)
================================================================================

╔══════════════════════════════════════════════════════════════════════════════╗
║                              DATA LOADING                                   ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. READING FILES:
   df = pd.read_csv('data.csv')                    # Most common
   df = pd.read_excel('data.xlsx', sheet_name=0)
   df = pd.read_json('data.json')
   df = pd.read_sql('SELECT * FROM table', conn)
   df = pd.read_html('url')[0]                     # Read HTML tables

2. WITH OPTIONS (IMPORTANT!):
   df = pd.read_csv('data.csv',
                    sep=',',                      # Separator
                    header=0,                     # Row for column names
                    names=['col1', 'col2'],       # Custom column names
                    index_col=0,                  # Column as index
                    usecols=[0, 1, 3],            # Read specific columns
                    dtype={'col1': int},          # Specify dtypes
                    na_values=['NA', 'null'],     # Custom NA values
                    skiprows=2,                   # Skip rows
                    nrows=1000)                   # Read only n rows

3. SAVING DATA:
   df.to_csv('output.csv', index=False)           # Don't save index
   df.to_excel('output.xlsx', index=False)
   df.to_json('output.json')

╔══════════════════════════════════════════════════════════════════════════════╗
║                          DATA INSPECTION                                    ║
╚══════════════════════════════════════════════════════════════════════════════╝

df.head(n)                     → First n rows (default 5)
df.tail(n)                     → Last n rows
df.sample(n)                   → Random n rows
df.shape                       → (rows, columns)
df.info()                      → Data types, memory, non-null counts
df.describe()                  → Statistical summary
df.describe(include='all')     → Include categorical

df.columns                     → Column names
df.index                       → Index
df.dtypes                      → Data type per column
df.select_dtypes(include=['int64', 'float64'])  → Select numeric

df.isnull().sum()              → Count missing per column
df.duplicated().sum()          → Count duplicates
df.nunique()                   → Unique values per column
df['col'].value_counts()       → Frequency of values
df['col'].unique()             → Array of unique values
df.memory_usage()              → Memory usage per column

╔══════════════════════════════════════════════════════════════════════════════╗
║                        DATA SELECTION (CRITICAL!)                           ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. COLUMN SELECTION:
   df['column']                → Single column (Series)
   df[['col1', 'col2']]        → Multiple columns (DataFrame)
   df.column_name              → Dot notation (if no spaces)

2. ROW SELECTION BY LABEL (.loc):
   df.loc[row_label]           → Single row
   df.loc[start:stop]          → Slice of rows (INCLUSIVE!)
   df.loc[[label1, label2]]    → Multiple rows
   df.loc[row_label, 'col']    → Specific cell
   df.loc[condition]           → Boolean indexing

3. ROW SELECTION BY POSITION (.iloc):
   df.iloc[row_idx]            → Single row by position
   df.iloc[start:stop]         → Slice (EXCLUSIVE at end!)
   df.iloc[[idx1, idx2]]       → Multiple rows
   df.iloc[row_idx, col_idx]   → Specific cell
   df.iloc[1:5, 2:4]           → Rows 1-4, columns 2-3

4. BOOLEAN INDEXING (ML IMPORTANT):
   df[df['age'] > 30]                          → Simple condition
   df[(df['age'] > 30) & (df['city'] == 'Dhaka')]  → AND condition
   df[(df['age'] > 30) | (df['salary'] > 50000)]   → OR condition
   df[~df['name'].str.contains('A')]               → NOT condition
   df[df['col'].isin(['val1', 'val2'])]            → Is in list
   df[df['col'].between(20, 30)]                   → Between values

5. QUERY METHOD (Cleaner syntax):
   df.query('age > 30 and city == "Dhaka"')
   df.query('salary > @threshold')                # Use variable

╔══════════════════════════════════════════════════════════════════════════════╗
║                        DATA CLEANING & PREPROCESSING                        ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. HANDLING MISSING VALUES:
   df.dropna()                          → Drop rows with any NaN
   df.dropna(axis=1)                    → Drop columns with any NaN
   df.dropna(how='all')                 → Drop rows where ALL are NaN
   df.dropna(subset=['col1', 'col2'])   → Drop rows where specified cols NaN

   df.fillna(0)                         → Fill NaN with 0
   df.fillna(df.mean())                 → Fill with column mean
   df.fillna(method='ffill')            → Forward fill
   df.fillna(method='bfill')            → Backward fill
   df.interpolate()                     → Interpolate

2. REMOVING DUPLICATES:
   df.drop_duplicates()                 → Remove duplicate rows
   df.drop_duplicates(subset=['col'])   → Remove based on column
   df.drop_duplicates(keep='first')     → Keep first occurrence
   df.drop_duplicates(keep='last')      → Keep last occurrence

3. RENAMING:
   df.rename(columns={'old':'new'})
   df.rename(index={'old':'new'})
   df.columns = ['col1', 'col2', 'col3']  → Bulk rename

4. TYPE CONVERSION:
   df['col'] = df['col'].astype(int)
   df['col'] = pd.to_numeric(df['col'], errors='coerce')
   df['date'] = pd.to_datetime(df['date'])
   pd.get_dummies(df['categorical_col'])  → One-hot encoding

5. STRING OPERATIONS (via .str):
   df['col'].str.upper() / .str.lower()
   df['col'].str.strip()                → Remove whitespace
   df['col'].str.contains('pattern')
   df['col'].str.replace('old', 'new')
   df['col'].str.split(' ', expand=True)
   df['col'].str.len()

╔══════════════════════════════════════════════════════════════════════════════╗
║                         DATA TRANSFORMATION                                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. ADDING/REMOVING COLUMNS:
   df['new_col'] = values                     → Add column
   df.insert(loc, 'new_col', values)          → Insert at position
   df.drop('col', axis=1)                     → Remove column
   df.drop(['col1', 'col2'], axis=1)          → Remove multiple
   df.pop('col')                              → Remove and return column

2. APPLY FUNCTIONS:
   df['col'].apply(lambda x: x*2)             → Apply to column
   df.apply(lambda row: row['a'] + row['b'], axis=1) → Apply to rows
   df.applymap(lambda x: len(str(x)))         → Apply to all elements

3. GROUPBY OPERATIONS (IMPORTANT FOR ML):
   grouped = df.groupby('category')
   grouped.size()                             → Count per group
   grouped.mean()                             → Mean per group
   grouped.sum()                              → Sum per group
   grouped.agg({'col1':'mean', 'col2':'sum'}) → Multiple aggregations returns (n,) where n = number of category
   grouped.transform('mean')                  → Transform values returns (n,) where n = number of rows in the frame

4. PIVOT TABLES:
   pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])

5. MELT (Wide to Long):
   pd.melt(df, id_vars=['id'], value_vars=['A', 'B'])

╔══════════════════════════════════════════════════════════════════════════════╗
║                         COMBINING DATAFRAMES                                ║
╚══════════════════════════════════════════════════════════════════════════════╝

pd.concat([df1, df2])                    → Stack vertically
pd.concat([df1, df2], axis=1)            → Stack horizontally
pd.concat([df1, df2], ignore_index=True) → Reset index

pd.merge(df1, df2, on='key')             → SQL-like join
pd.merge(df1, df2, left_on='lkey', right_on='rkey')
pd.merge(df1, df2, how='inner')          → Inner join (default)
pd.merge(df1, df2, how='left')           → Left join
pd.merge(df1, df2, how='right')          → Right join
pd.merge(df1, df2, how='outer')          → Full outer join

df1.join(df2)                            → Join on index

╔══════════════════════════════════════════════════════════════════════════════╗
║                         TIME SERIES (FOR TIME-DEPENDENT ML)                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
df.resample('D').mean()                   → Daily resample
df.resample('M').sum()                    → Monthly resample
df.rolling(window=7).mean()               → 7-day moving average
df.shift(1)                               → Shift values by 1 period
df.diff()                                 → Difference with previous

╔══════════════════════════════════════════════════════════════════════════════╗
║                         ML-SPECIFIC OPERATIONS                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. TRAIN-TEST SPLIT:
   from sklearn.model_selection import train_test_split
   X_train, X_test, y_train, y_test = train_test_split(
       X, y, test_size=0.2, random_state=42
   )

2. FEATURE SCALING:
   from sklearn.preprocessing import StandardScaler
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X)

3. ONE-HOT ENCODING:
   df_encoded = pd.get_dummies(df, columns=['categorical_col'])

4. HANDLING IMBALANCED DATA:
   from imblearn.over_sampling import SMOTE
   smote = SMOTE(random_state=42)
   X_res, y_res = smote.fit_resample(X, y)

5. CROSS-VALIDATION:
   from sklearn.model_selection import KFold
   kf = KFold(n_splits=5, shuffle=True, random_state=42)

╔══════════════════════════════════════════════════════════════════════════════╗
║                         PERFORMANCE TIPS FOR EXAM                           ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. VECTORIZED OPERATIONS (FASTER):
   # SLOW: df['new'] = df.apply(lambda row: row['a'] + row['b'], axis=1)
   # FAST: df['new'] = df['a'] + df['b']

2. USE .loc/.iloc CORRECTLY:
   # Wrong: df[0] (for row selection)
   # Correct: df.iloc[0] or df.loc[index_label]

3. CHAINING OPERATIONS:
   result = (df[df['age'] > 30]
             .groupby('city')['salary']
             .mean()
             .sort_values(ascending=False))

4. MEMORY EFFICIENCY:
   df = df.astype({'col': 'int32'})  # Use smaller dtype
   df = df.select_dtypes(include=['number'])  # Keep only numeric

5. HANDLE LARGE DATASETS:
   # Read in chunks
   chunk_size = 10000
   for chunk in pd.read_csv('large.csv', chunksize=chunk_size):
       process(chunk)

================================================================================
                      BUET ML EXAM - PANDAS KEY POINTS
================================================================================
• Remember: .loc is label-based, .iloc is position-based
• Use df.copy() to avoid SettingWithCopyWarning
• Prefer vectorized operations over apply() for speed
• Check df.info() after loading to see memory usage
• Use pd.get_dummies() for categorical encoding
• Always set random_state for reproducibility
================================================================================
