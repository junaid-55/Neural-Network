================================================================================
                       PYTORCH CHEATSHEET FOR ML/DL (BUET EXAM)
================================================================================

╔══════════════════════════════════════════════════════════════════════════════╗
║                              TENSOR CREATION                                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. BASIC TENSORS:
   torch.tensor([1,2,3])                    → Tensor from list/array
   torch.zeros((m,n))                       → m×n tensor of zeros
   torch.ones((m,n))                        → m×n tensor of ones
   torch.full((m,n), value)                 → Fill with specified value
   torch.eye(n)                             → n×n identity matrix
   torch.arange(start, stop, step)          → Like range() but returns tensor
   torch.linspace(start, stop, num)         → num equally spaced values
   torch.rand(m,n)                          → Uniform random [0,1)
   torch.randn(m,n)                         → Normal random (μ=0, σ=1)
   torch.randint(low, high, size)           → Random integers

2. IMPORTANT FOR ML/DL:
   X = torch.randn(100, 10)                 # 100 samples, 10 features
   y = torch.randint(0, 2, (100,))          # Binary labels
   weights = torch.zeros(10, requires_grad=True)

3. FROM NUMPY:
   torch.from_numpy(np_array)               → Tensor from numpy array
   tensor.numpy()                           → Convert back to numpy

4. DEVICE MANAGEMENT:
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   tensor.to(device)                        → Move tensor to device

╔══════════════════════════════════════════════════════════════════════════════╗
║                          TENSOR PROPERTIES & INFO                            ║
╚══════════════════════════════════════════════════════════════════════════════╝

tensor.shape                                → torch.Size object
tensor.ndim                                 → Number of dimensions
tensor.numel()                              → Total number of elements
tensor.dtype                                → Data type (torch.float32, etc.)
tensor.device                               → Device (cpu/cuda)
tensor.requires_grad                        → Gradient tracking enabled
tensor.grad                                 → Gradient (if computed)

tensor.T or tensor.t()                      → Transpose (for 2D)
tensor.reshape(new_shape)                   → Reshape
tensor.view(new_shape)                      → View (must be contiguous)
tensor.unsqueeze(dim)                       → Add dimension
tensor.squeeze(dim)                         → Remove dimension of size 1
tensor.flatten(start_dim=0, end_dim=-1)     → Flatten tensor

╔══════════════════════════════════════════════════════════════════════════════╗
║                         INDEXING & SLICING (CRITICAL!)                       ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. BASIC INDEXING:
   tensor[0]                                → First element
   tensor[0, 1]                             → Element at row 0, column 1
   tensor[:, 1]                             → All rows, column 1
   tensor[1, :]                             → Row 1, all columns

2. SLICING:
   tensor[start:stop:step]                  → Basic slice
   tensor[:3]                               → First 3 rows
   tensor[-3:]                              → Last 3 rows
   tensor[:, ::2]                           → Every other column

3. ADVANCED INDEXING:
   tensor[[0, 2, 4]]                        → Rows 0, 2, 4
   torch.masked_select(tensor, mask)        → Select with boolean mask

╔══════════════════════════════════════════════════════════════════════════════╗
║                     TENSOR OPERATIONS (ML/DL FOCUSED)                        ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. ELEMENT-WISE OPERATIONS:
   tensor1 + tensor2, tensor1 - tensor2, tensor1 * tensor2, tensor1 / tensor2
   tensor ** 2                              → Square each element
   torch.sqrt(tensor), torch.exp(tensor), torch.log(tensor), torch.abs(tensor)
   torch.sigmoid(tensor), torch.relu(tensor), torch.tanh(tensor)

2. REDUCTION OPERATIONS:
   tensor.sum(), tensor.mean(), tensor.std(), tensor.var()
   tensor.min(), tensor.max(), tensor.argmin(), tensor.argmax()

3. MATRIX OPERATIONS:
   torch.matmul(A, B) or A @ B              → Matrix multiplication
   torch.mm(A, B)                           → Matrix multiplication (2D only)
   torch.inverse(A)                         → Matrix inverse
   torch.det(A)                             → Determinant
   torch.norm(x)                            → Vector norm (L2 by default)

╔══════════════════════════════════════════════════════════════════════════════╗
║                              CNN LAYERS                                      ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. CONVOLUTIONAL LAYERS:
   nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)
   
   # Common patterns:
   nn.Conv2d(1, 32, 3, padding=1)          # Keep same size (MNIST: 28→28)
   nn.Conv2d(32, 64, 3, stride=2, padding=1)  # Halve size (14→7)
   
   # Output size formula:
   # New = floor((W + 2P - K)/S + 1)

2. POOLING LAYERS:
   nn.MaxPool2d(kernel_size, stride=None)   # Default stride = kernel_size
   nn.AvgPool2d(kernel_size, stride=None)
   nn.AdaptiveAvgPool2d(output_size)        # Output fixed size
   
   # Common usage:
   nn.MaxPool2d(2, 2)                       # Halves dimensions
   nn.AdaptiveAvgPool2d((1, 1))             # Global Average Pooling

3. BATCH NORMALIZATION:
   nn.BatchNorm2d(num_features)             # num_features = conv output channels
   # Placement: Conv → BatchNorm → ReLU

4. ACTIVATION FUNCTIONS:
   nn.ReLU()                                # Most common for CNN
   nn.LeakyReLU(negative_slope=0.01)
   nn.Sigmoid(), nn.Tanh()
   nn.Softmax(dim=1)                        # For classification output

╔══════════════════════════════════════════════════════════════════════════════╗
║                          NEURAL NETWORK MODULES                             ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. DEFINING MODELS:
   class CNN(nn.Module):
       def __init__(self):
           super().__init__()
           self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
           self.bn1 = nn.BatchNorm2d(32)
           self.relu = nn.ReLU()
           self.pool = nn.MaxPool2d(2, 2)
           self.fc = nn.Linear(32 * 14 * 14, 10)  # Calculate based on input!
       
       def forward(self, x):
           x = self.relu(self.bn1(self.conv1(x)))
           x = self.pool(x)
           x = x.view(x.size(0), -1)        # Flatten
           x = self.fc(x)
           return x

2. SEQUENTIAL MODELS:
   model = nn.Sequential(
       nn.Conv2d(1, 32, 3, padding=1),
       nn.BatchNorm2d(32),
       nn.ReLU(),
       nn.MaxPool2d(2, 2),
       nn.Flatten(),
       nn.Linear(32 * 14 * 14, 10)
   )

3. COMMON LAYERS:
   nn.Linear(in_features, out_features)     → Fully connected
   nn.Conv2d(in_c, out_c, kernel_size)      → 2D convolution
   nn.MaxPool2d(kernel_size, stride)        → Max pooling
   nn.BatchNorm2d(num_features)             → Batch normalization
   nn.Dropout(p=0.5)                        → Dropout regularization
   nn.Flatten()                             → Flatten for FC layers

╔══════════════════════════════════════════════════════════════════════════════╗
║                          AUTOGRAD & OPTIMIZATION                            ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. GRADIENT TRACKING:
   tensor.requires_grad_(True)              → Enable gradient tracking
   with torch.no_grad():                    → Disable gradient tracking
   loss.backward()                          → Compute gradients
   optimizer.zero_grad()                    → Zero gradients (CRITICAL!)

2. OPTIMIZERS:
   optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
   optimizer = optim.Adam(model.parameters(), lr=0.001)
   optimizer = optim.RMSprop(model.parameters(), lr=0.01)
   optimizer.step()                         → Update parameters

3. LOSS FUNCTIONS:
   criterion = nn.CrossEntropyLoss()        → For classification (MOST COMMON)
   criterion = nn.MSELoss()                 → Mean Squared Error (regression)
   criterion = nn.BCELoss()                 → Binary Cross Entropy
   criterion = nn.L1Loss()                  → Mean Absolute Error

╔══════════════════════════════════════════════════════════════════════════════╗
║                          TRANSFORMS FOR IMAGES                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

from torchvision import transforms

1. BASIC TRANSFORMS (for training)
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),           # Resize to fixed size
    transforms.RandomHorizontalFlip(p=0.5),  # Data augmentation
    transforms.RandomRotation(10),           # Random rotation
    transforms.ColorJitter(                  # Color variations
        brightness=0.2, 
        contrast=0.2, 
        saturation=0.2
    ),
    transforms.RandomAffine(                 # Affine transformations
        degrees=0, 
        translate=(0.1, 0.1), 
        scale=(0.9, 1.1)
    ),
    transforms.ToTensor(),                   # Convert to tensor [0, 1]
    transforms.Normalize(                    # Standardize
        mean=[0.485, 0.456, 0.406],         # ImageNet stats
        std=[0.229, 0.224, 0.225]
    ),
])

2. VALIDATION/TEST TRANSFORMS (no augmentation)
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

3. GRAYSCALE TRANSFORMS (for MNIST)
mnist_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # MNIST stats
])

4. CIFAR-10 TRANSFORMS
cifar_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2470, 0.2435, 0.2616]
    )
])

╔══════════════════════════════════════════════════════════════════════════════╗
║                         DATA LOADING FOR MNIST                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. MNIST TRANSFORMS:
   from torchvision import transforms, datasets
   
   # Basic transform (no resizing needed for MNIST)
   transform = transforms.Compose([
       transforms.ToTensor(),                    # Convert PIL to tensor [0,1]
       transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean/std
   ])
   
   # With augmentation (for better generalization)
   train_transform = transforms.Compose([
       transforms.RandomRotation(10),           # Small rotations
       transforms.ToTensor(),
       transforms.Normalize((0.1307,), (0.3081,))
   ])

2. DATASET LOADING:
   train_dataset = datasets.MNIST(
       root='./data',                    # Download directory
       train=True,                       # Training set
       download=True,                    # Download if not exists
       transform=transform
   )
   
   test_dataset = datasets.MNIST(
       root='./data',
       train=False,                      # Test set
       download=True,
       transform=transform
   )

3. DATALOADER WITH PROPER ATTRIBUTES:
   from torch.utils.data import DataLoader, random_split
   
   # Train/Validation split
   train_size = int(0.8 * len(train_dataset))
   val_size = len(train_dataset) - train_size
   train_subset, val_subset = random_split(train_dataset, [train_size, val_size])
   
   # Training dataloader
   train_loader = DataLoader(
       dataset=train_subset,
       batch_size=32,                    # Common: 32, 64, 128
       shuffle=True,                     # MUST shuffle for training
       num_workers=2,                    # Parallel loading
       pin_memory=True,                  # Faster GPU transfer
       drop_last=True                    # Drop incomplete batch
   )
   
   # Validation dataloader
   val_loader = DataLoader(
       dataset=val_subset,
       batch_size=32,
       shuffle=False,                    # No shuffle for validation
       num_workers=2,
       pin_memory=True,
       drop_last=False                   # Use all validation data
   )
   
   # Test dataloader
   test_loader = DataLoader(
       dataset=test_dataset,
       batch_size=32,
       shuffle=False,
       num_workers=2,
       pin_memory=False
   )

4. CUSTOM DATASET (if needed):
   class CustomDataset(torch.utils.data.Dataset):
       def __init__(self, data, labels, transform=None):
           self.data = data
           self.labels = labels
           self.transform = transform
       
       def __len__(self):
           return len(self.data)
       
       def __getitem__(self, idx):
           sample = self.data[idx]
           label = self.labels[idx]
           
           if self.transform:
               sample = self.transform(sample)
           
           return sample, label

╔══════════════════════════════════════════════════════════════════════════════╗
║                         TRAINING LOOP (CRITICAL!)                           ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. BASIC TRAINING LOOP:
   model.train()                           → Set model to training mode
   for epoch in range(num_epochs):
       for images, labels in train_loader:
           images, labels = images.to(device), labels.to(device)
           
           # Forward pass
           outputs = model(images)
           loss = criterion(outputs, labels)
           
           # Backward pass and optimize
           optimizer.zero_grad()           # CLEAR GRADIENTS!
           loss.backward()
           optimizer.step()
       
       print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

2. WITH ACCURACY CALCULATION:
   model.train()
   running_loss = 0.0
   correct = 0
   total = 0
   
   for images, labels in train_loader:
       images, labels = images.to(device), labels.to(device)
       
       optimizer.zero_grad()
       outputs = model(images)
       loss = criterion(outputs, labels)
       loss.backward()
       optimizer.step()
       
       # Calculate accuracy
       _, predicted = outputs.max(1)
       total += labels.size(0)
       correct += predicted.eq(labels).sum().item()
       running_loss += loss.item()
   
   train_loss = running_loss / len(train_loader)
   train_acc = 100. * correct / total

3. VALIDATION LOOP:
   model.eval()                            → Set model to evaluation mode
   val_loss = 0.0
   correct = 0
   total = 0
   
   with torch.no_grad():                   → Disable gradient computation
       for images, labels in val_loader:
           images, labels = images.to(device), labels.to(device)
           outputs = model(images)
           loss = criterion(outputs, labels)
           
           val_loss += loss.item()
           _, predicted = outputs.max(1)
           total += labels.size(0)
           correct += predicted.eq(labels).sum().item()
   
   val_acc = 100. * correct / total
   print(f'Validation Loss: {val_loss/len(val_loader):.4f}, '
         f'Accuracy: {val_acc:.2f}%')

╔══════════════════════════════════════════════════════════════════════════════╗
║                         UTILITY FUNCTIONS                                   ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. MODEL SAVING/LOADING:
   torch.save(model.state_dict(), 'model.pth')
   model.load_state_dict(torch.load('model.pth'))
   
   # Save checkpoint
   torch.save({
       'epoch': epoch,
       'model_state_dict': model.state_dict(),
       'optimizer_state_dict': optimizer.state_dict(),
       'loss': loss,
   }, 'checkpoint.pth')

2. TENSOR OPERATIONS:
   torch.cat(tensors, dim=0)               → Concatenate along dimension
   torch.stack(tensors, dim=0)             → Stack along new dimension
   torch.split(tensor, split_size, dim=0)  → Split tensor

3. REPRODUCIBILITY:
   torch.manual_seed(42)
   torch.cuda.manual_seed_all(42)

4. GRADIENT CLIPPING:
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

╔══════════════════════════════════════════════════════════════════════════════╗
║                         COMMON CNN PATTERNS                                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. BASIC CNN FOR MNIST:
   class MNISTCNN(nn.Module):
       def __init__(self):
           super().__init__()
           self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # 28→28
           self.bn1 = nn.BatchNorm2d(32)
           self.pool1 = nn.MaxPool2d(2, 2)               # 28→14
           
           self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 14→14
           self.bn2 = nn.BatchNorm2d(64)
           self.pool2 = nn.MaxPool2d(2, 2)               # 14→7
           
           self.fc1 = nn.Linear(64 * 7 * 7, 128)
           self.dropout = nn.Dropout(0.5)
           self.fc2 = nn.Linear(128, 10)
       
       def forward(self, x):
           x = F.relu(self.bn1(self.conv1(x)))
           x = self.pool1(x)
           x = F.relu(self.bn2(self.conv2(x)))
           x = self.pool2(x)
           x = x.view(x.size(0), -1)  # Flatten
           x = F.relu(self.fc1(x))
           x = self.dropout(x)
           x = self.fc2(x)
           return x

2. WITH GLOBAL AVERAGE POOLING:
   class SimpleCNN(nn.Module):
       def __init__(self):
           super().__init__()
           self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
           self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
           self.gap = nn.AdaptiveAvgPool2d((1, 1))
           self.fc = nn.Linear(64, 10)
       
       def forward(self, x):
           x = F.relu(self.conv1(x))
           x = F.relu(self.conv2(x))
           x = self.gap(x)
           x = x.view(x.size(0), -1)
           x = self.fc(x)
           return x

3. CALCULATING OUTPUT SIZES:
   def calculate_output_size(input_size, kernel_size, stride=1, padding=0):
       """Calculate CNN output size"""
       return (input_size + 2*padding - kernel_size) // stride + 1
   
   # Example: MNIST flow
   # Input: 28×28
   # After conv1(3,padding=1,stride=1): 28 (same)
   # After pool1(2,stride=2): 14
   # After conv2(3,padding=1,stride=1): 14
   # After pool2(2,stride=2): 7
   # Flatten: 64×7×7 = 3136
   # fc1 input: 3136

╔══════════════════════════════════════════════════════════════════════════════╗
║                         COMMON PITFALLS & SOLUTIONS                         ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. FORGETTING optimizer.zero_grad():
   # CORRECT:
   optimizer.zero_grad()
   loss.backward()
   optimizer.step()

2. WRONG DEVICE:
   # CORRECT:
   model = model.to(device)
   images = images.to(device)
   labels = labels.to(device)

3. NOT SETTING model.train()/model.eval():
   # CORRECT:
   model.train()    # During training
   model.eval()     # During evaluation

4. DIMENSION CALCULATION ERROR:
   # WRONG for MNIST after two 2×2 pools:
   nn.Linear(64 * 14 * 14, 128)  # Wrong! Should be 7×7
   # CORRECT:
   nn.Linear(64 * 7 * 7, 128)    # 28→14→7

5. NOT NORMALIZING DATA:
   # Always normalize MNIST:
   transforms.Normalize((0.1307,), (0.3081,))

================================================================================
                      BUET ML/DL EXAM - PYTORCH KEY POINTS
================================================================================
• Conv2d output size: floor((W + 2P - K)/S + 1)
• For MNIST: mean=0.1307, std=0.3081
• Always: optimizer.zero_grad() before loss.backward()
• Use model.train() and model.eval() correctly
• Move both model and data to same device
• Check tensor shapes: print(x.shape)
• Calculate CNN dimensions for Linear layer input
• Shuffle=True for training, False for validation/test
• Use nn.CrossEntropyLoss() for classification
• Set random seeds: torch.manual_seed(42)
• MNIST: 60,000 training, 10,000 test, 28×28 grayscale
================================================================================