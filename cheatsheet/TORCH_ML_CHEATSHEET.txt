================================================================================
                       PYTORCH CHEATSHEET FOR ML/DL (BUET EXAM)
================================================================================

╔══════════════════════════════════════════════════════════════════════════════╗
║                              TENSOR CREATION                                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. BASIC TENSORS:
   torch.tensor([1,2,3])                    → Tensor from list/array
   torch.zeros((m,n))                       → m×n tensor of zeros
   torch.ones((m,n))                        → m×n tensor of ones
   torch.full((m,n), value)                 → Fill with specified value
   torch.eye(n)                             → n×n identity matrix
   torch.arange(start, stop, step)          → Like range() but returns tensor
   torch.linspace(start, stop, num)         → num equally spaced values
   torch.rand(m,n)                          → Uniform random [0,1)
   torch.randn(m,n)                         → Normal random (μ=0, σ=1)
   torch.randint(low, high, size)           → Random integers
   torch.empty(m,n)                         → Uninitialized tensor (WARNING: may contain garbage)

2. IMPORTANT FOR ML/DL:
   X = torch.randn(100, 10)                 # 100 samples, 10 features
   y = torch.randint(0, 2, (100,))          # Binary labels
   weights = torch.zeros(10, requires_grad=True)  # For gradient tracking
   bias = torch.zeros(1, requires_grad=True)

3. FROM NUMPY (CRITICAL FOR DATA LOADING):
   torch.from_numpy(np_array)               → Tensor from numpy array (shares memory)
   tensor.numpy()                           → Convert back to numpy

4. DEVICE MANAGEMENT (GPU/CPU):
   torch.device('cuda' if torch.cuda.is_available() else 'cpu')
   tensor.to(device)                        → Move tensor to device
   torch.cuda.is_available()                → Check GPU availability

╔══════════════════════════════════════════════════════════════════════════════╗
║                          TENSOR PROPERTIES & INFO                            ║
╚══════════════════════════════════════════════════════════════════════════════╝

tensor.shape                                → torch.Size object (like tuple)
tensor.ndim                                 → Number of dimensions
tensor.numel()                              → Total number of elements
tensor.dtype                                → Data type (torch.float32, etc.)
tensor.device                               → Device (cpu/cuda)
tensor.requires_grad                        → Whether gradient tracking is enabled
tensor.grad                                 → Gradient (if computed)
tensor.grad_fn                              → Function that created this tensor

tensor.T or tensor.t()                      → Transpose (for 2D)
tensor.permute(dims)                        → Permute dimensions (more flexible)
tensor.reshape(new_shape)                   → Reshape (may create view)
tensor.view(new_shape)                      → View (must be contiguous)
tensor.unsqueeze(dim)                       → Add dimension at position dim
tensor.squeeze(dim)                         → Remove dimension of size 1
tensor.flatten(start_dim=0, end_dim=-1)     → Flatten tensor

EXAMPLES:
   X.shape      → torch.Size([100, 10])
   X.T.shape    → torch.Size([10, 100])
   X.unsqueeze(0).shape → torch.Size([1, 100, 10])  # Add batch dimension

╔══════════════════════════════════════════════════════════════════════════════╗
║                         INDEXING & SLICING (CRITICAL!)                       ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. BASIC INDEXING (SAME AS NUMPY):
   tensor[0]                                → First element (row if 2D)
   tensor[0, 1]                             → Element at row 0, column 1
   tensor[:, 1]                             → All rows, column 1
   tensor[1, :]                             → Row 1, all columns

2. SLICING:
   tensor[start:stop:step]                  → Basic slice
   tensor[:3]                               → First 3 rows
   tensor[-3:]                              → Last 3 rows
   tensor[:, ::2]                           → Every other column
   tensor[...]                              → Ellipsis for multiple dimensions

3. ADVANCED INDEXING:
   tensor[[0, 2, 4]]                        → Rows 0, 2, 4
   tensor[:, [1, 3, 5]]                     → Columns 1, 3, 5
   torch.masked_select(tensor, mask)        → Select with boolean mask
   torch.nonzero(tensor)                    → Indices of non-zero elements

4. IN-PLACE OPERATIONS (WARNING: careful with autograd):
   tensor[index] = value                    → In-place modification
   tensor.add_(value)                       → In-place addition
   tensor.mul_(value)                       → In-place multiplication

╔══════════════════════════════════════════════════════════════════════════════╗
║                     TENSOR OPERATIONS (ML/DL FOCUSED)                        ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. ELEMENT-WISE OPERATIONS:
   tensor1 + tensor2, tensor1 - tensor2, tensor1 * tensor2, tensor1 / tensor2
   tensor ** 2                              → Square each element
   torch.sqrt(tensor)                       → Square root
   torch.exp(tensor)                        → Exponential
   torch.log(tensor)                        → Natural log
   torch.abs(tensor)                        → Absolute value
   torch.sigmoid(tensor)                    → Sigmoid activation
   torch.relu(tensor)                       → ReLU activation
   torch.tanh(tensor)                       → Tanh activation

2. REDUCTION OPERATIONS (for loss functions):
   tensor.sum()                             → Sum of all elements
   tensor.mean()                            → Mean (average)
   tensor.std()                             → Standard deviation
   tensor.var()                             → Variance
   tensor.min(), tensor.max()               → Min and max
   tensor.argmin(), tensor.argmax()         → Index of min/max
   torch.median(tensor)                     → Median
   torch.topk(tensor, k)                    → Top k values

3. MATRIX OPERATIONS (CRITICAL FOR LINEAR LAYERS):
   torch.matmul(A, B) or A @ B              → Matrix multiplication
   torch.mm(A, B)                           → Matrix multiplication (2D only)
   torch.bmm(A, B)                          → Batch matrix multiplication
   torch.inverse(A)                         → Matrix inverse
   torch.pinverse(A)                        → Pseudo-inverse
   torch.det(A)                             → Determinant
   torch.eig(A)                             → Eigenvalues/vectors
   torch.norm(x)                            → Vector norm (L2 by default)
   torch.norm(x, p=1)                       → L1 norm
   torch.dot(a, b)                          → Dot product (1D tensors)

4. CONVOLUTION OPERATIONS (FOR CNNs):
   torch.conv1d(), torch.conv2d(), torch.conv3d()
   torch.nn.functional.conv2d(input, weight, bias)

5. BROADCASTING RULES (SAME AS NUMPY):
   • Trailing dimensions must match OR be 1
   Example: (3,4) + (4,) works → (4,) becomes (1,4)
   Example: (2,3) + (3,1) works

╔══════════════════════════════════════════════════════════════════════════════╗
║                          AUTOGRAD & OPTIMIZATION                            ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. GRADIENT TRACKING:
   tensor.requires_grad_(True)              → Enable gradient tracking
   with torch.no_grad():                    → Disable gradient tracking
   tensor.detach()                          → Detach from computation graph
   tensor.retain_grad()                     → Retain gradient for non-leaf tensors

2. COMPUTING GRADIENTS:
   loss.backward()                          → Compute gradients
   optimizer.zero_grad()                    → Zero gradients (CRITICAL!)

3. OPTIMIZERS:
   import torch.optim as optim
   optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
   optimizer = optim.Adam(model.parameters(), lr=0.001)
   optimizer = optim.RMSprop(model.parameters(), lr=0.01)
   
   optimizer.step()                         → Update parameters
   optimizer.zero_grad()                    → Clear gradients

4. LOSS FUNCTIONS:
   import torch.nn as nn
   criterion = nn.MSELoss()                 → Mean Squared Error
   criterion = nn.CrossEntropyLoss()        → Cross Entropy (for classification)
   criterion = nn.BCELoss()                 → Binary Cross Entropy
   criterion = nn.BCEWithLogitsLoss()       → BCE with sigmoid
   criterion = nn.L1Loss()                  → Mean Absolute Error

   loss = criterion(output, target)
   loss.backward()

╔══════════════════════════════════════════════════════════════════════════════╗
║                         NEURAL NETWORK MODULES                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. DEFINING MODELS:
   class Net(nn.Module):
       def __init__(self):
           super().__init__()
           self.fc1 = nn.Linear(10, 5)     # Input: 10, Output: 5
           self.fc2 = nn.Linear(5, 2)      # Input: 5, Output: 2
           self.relu = nn.ReLU()
           self.dropout = nn.Dropout(p=0.5)
       
       def forward(self, x):
           x = self.relu(self.fc1(x))
           x = self.dropout(x)
           x = self.fc2(x)
           return x
   
   model = Net()
   model.to(device)                        → Move model to device

2. COMMON LAYERS:
   nn.Linear(in_features, out_features)    → Fully connected
   nn.Conv2d(in_channels, out_channels, kernel_size)
   nn.Conv1d(in_channels, out_channels, kernel_size)
   nn.MaxPool2d(kernel_size, stride)       → Max pooling
   nn.AvgPool2d(kernel_size, stride)       → Average pooling
   nn.BatchNorm2d(num_features)            → Batch normalization
   nn.Dropout(p=0.5)                       → Dropout
   nn.Embedding(vocab_size, embedding_dim) → Embedding layer

3. ACTIVATION FUNCTIONS:
   nn.ReLU(), nn.Sigmoid(), nn.Tanh()
   nn.LeakyReLU(negative_slope=0.01)
   nn.Softmax(dim=1)                       → Specify dimension
   nn.LogSoftmax(dim=1)                    → For CrossEntropyLoss

4. SEQUENTIAL MODELS:
   model = nn.Sequential(
       nn.Linear(10, 5),
       nn.ReLU(),
       nn.Linear(5, 2),
       nn.Softmax(dim=1)
   )

╔══════════════════════════════════════════════════════════════════════════════╗
║                         DATA LOADING & DATASETS                             ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. CUSTOM DATASETS:
   class CustomDataset(torch.utils.data.Dataset):
       def __init__(self, data, labels):
           self.data = data
           self.labels = labels
       
       def __len__(self):
           return len(self.data)
       
       def __getitem__(self, idx):
           sample = self.data[idx]
           label = self.labels[idx]
           return sample, label

2. DATALOADERS (CRITICAL FOR BATCHING):
   from torch.utils.data import DataLoader
   
   dataset = CustomDataset(X, y)
   dataloader = DataLoader(dataset, 
                          batch_size=32,
                          shuffle=True,
                          num_workers=2)   # For parallel loading
   
   for batch_X, batch_y in dataloader:
       batch_X, batch_y = batch_X.to(device), batch_y.to(device)
       # Training loop

3. TRANSFORMS (FOR DATA AUGMENTATION):
   from torchvision import transforms
   
   transform = transforms.Compose([
       transforms.ToTensor(),
       transforms.Normalize((0.5,), (0.5,)),
       transforms.RandomHorizontalFlip(),
       transforms.RandomCrop(32, padding=4)
   ])

4. BUILT-IN DATASETS:
   torchvision.datasets.MNIST()
   torchvision.datasets.CIFAR10()
   torchvision.datasets.ImageFolder()

╔══════════════════════════════════════════════════════════════════════════════╗
║                         TRAINING LOOP (CRITICAL!)                           ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. BASIC TRAINING LOOP:
   model.train()                           → Set model to training mode
   for epoch in range(num_epochs):
       for batch_X, batch_y in train_loader:
           batch_X, batch_y = batch_X.to(device), batch_y.to(device)
           
           # Forward pass
           outputs = model(batch_X)
           loss = criterion(outputs, batch_y)
           
           # Backward pass and optimize
           optimizer.zero_grad()           # CLEAR GRADIENTS!
           loss.backward()                 # Compute gradients
           optimizer.step()                # Update weights
       
       print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

2. EVALUATION LOOP:
   model.eval()                            → Set model to evaluation mode
   with torch.no_grad():                   → Disable gradient computation
       correct = 0
       total = 0
       for batch_X, batch_y in test_loader:
           batch_X, batch_y = batch_X.to(device), batch_y.to(device)
           outputs = model(batch_X)
           _, predicted = torch.max(outputs.data, 1)
           total += batch_y.size(0)
           correct += (predicted == batch_y).sum().item()
       
       accuracy = 100 * correct / total
       print(f'Accuracy: {accuracy:.2f}%')

3. SAVING AND LOADING MODELS:
   torch.save(model.state_dict(), 'model.pth')
   model.load_state_dict(torch.load('model.pth'))
   
   # Save entire model
   torch.save(model, 'model_complete.pth')
   model = torch.load('model_complete.pth')
   
   # Save optimizer state
   torch.save({
       'epoch': epoch,
       'model_state_dict': model.state_dict(),
       'optimizer_state_dict': optimizer.state_dict(),
       'loss': loss,
   }, 'checkpoint.pth')

╔══════════════════════════════════════════════════════════════════════════════╗
║                         UTILITY FUNCTIONS                                   ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. TENSOR CONCATENATION & STACKING:
   torch.cat(tensors, dim=0)               → Concatenate along dimension
   torch.stack(tensors, dim=0)             → Stack along new dimension
   torch.split(tensor, split_size, dim=0)  → Split tensor
   torch.chunk(tensor, chunks, dim=0)      → Split into chunks

2. RANDOM SEEDS FOR REPRODUCIBILITY:
   torch.manual_seed(42)
   torch.cuda.manual_seed_all(42)

3. GRADIENT CLIPPING (FOR RNNs):
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

4. LEARNING RATE SCHEDULING:
   scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
   scheduler.step()                        → Call after optimizer.step()

╔══════════════════════════════════════════════════════════════════════════════╗
║                         COMMON ML/DL PATTERNS                               ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. SOFTMAX FUNCTION:
   def softmax(x, dim=1):
       return torch.exp(x) / torch.exp(x).sum(dim=dim, keepdim=True)

2. CROSS-ENTROPY FROM SCRATCH:
   def cross_entropy(outputs, labels):
       log_softmax = torch.log_softmax(outputs, dim=1)
       loss = -torch.sum(labels * log_softmax) / labels.size(0)
       return loss

3. WEIGHT INITIALIZATION:
   def init_weights(m):
       if isinstance(m, nn.Linear):
           nn.init.xavier_uniform_(m.weight)
           nn.init.zeros_(m.bias)
       elif isinstance(m, nn.Conv2d):
           nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
   
   model.apply(init_weights)

4. GRADIENT CHECKING:
   def check_gradient(model, input, target):
       output = model(input)
       loss = criterion(output, target)
       loss.backward()
       
       for name, param in model.named_parameters():
           if param.grad is not None:
               print(f'{name}: gradient mean={param.grad.mean().item():.6f}, std={param.grad.std().item():.6f}')

╔══════════════════════════════════════════════════════════════════════════════╗
║                         COMMON PITFALLS & SOLUTIONS                         ║
╚══════════════════════════════════════════════════════════════════════════════╝

1. FORGETTING optimizer.zero_grad():
   # WRONG: Causes gradient accumulation
   loss.backward()
   optimizer.step()
   
   # CORRECT: Clear gradients before each step
   optimizer.zero_grad()
   loss.backward()
   optimizer.step()

2. NOT MOVING TENSORS TO CORRECT DEVICE:
   # WRONG: Model on GPU, data on CPU
   model = model.to('cuda')
   output = model(input)  # input is on CPU
   
   # CORRECT: Move both to same device
   model = model.to(device)
   input = input.to(device)
   output = model(input)

3. NOT SETTING model.train()/model.eval():
   # WRONG: Dropout/BatchNorm behave incorrectly
   model = model.to(device)
   # During training:
   output = model(input)  # Without model.train()
   
   # CORRECT:
   model.train()  # During training
   output = model(input)
   
   model.eval()   # During evaluation
   with torch.no_grad():
       output = model(input)

4. IN-PLACE OPERATIONS ON GRADIENT-TRACKED TENSORS:
   # WRONG: Breaks computation graph
   x = torch.tensor([1.0], requires_grad=True)
   y = x + 2
   y.add_(1)  # In-place operation
   
   # CORRECT: Use out-of-place operations
   y = y + 1

5. MEMORY LEAKS IN TRAINING LOOP:
   # WRONG: Keeping references to tensors
   losses = []
   for batch in dataloader:
       loss = model(batch)
       losses.append(loss)  # Keeps computation graph alive
   
   # CORRECT: Detach and move to CPU
   losses.append(loss.item())  # or loss.detach().cpu().item()

================================================================================
                      BUET ML/DL EXAM - PYTORCH KEY POINTS
================================================================================
• Always call optimizer.zero_grad() before loss.backward()
• Use model.train() and model.eval() to toggle dropout/batchnorm
• Move both model and data to the same device (CPU/GPU)
• Use torch.no_grad() during inference to save memory
• Use .detach() when you need values without gradient tracking
• Prefer torch.nn.functional for stateless operations
• Use DataLoader for efficient batching and shuffling
• Set random seeds for reproducibility: torch.manual_seed(42)
• Check tensor shapes constantly: print(x.shape)
• Use tensor.item() to get Python scalar from 1-element tensor
================================================================================